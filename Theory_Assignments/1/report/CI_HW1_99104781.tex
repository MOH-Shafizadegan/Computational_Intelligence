\documentclass[]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{color}
\usepackage{listings}
\usepackage[top=1in, right=0.75in, left=0.75in]{geometry}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}

\definecolor{customgreen}{rgb}{0,0.6,0}
\definecolor{customgray}{rgb}{0.5,0.5,0.5}
\definecolor{custommauve}{rgb}{0.6,0,0.8}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
	language=MATLAB,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	frame=single,	                   % adds a frame around the code
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	rulecolor=\color{black},
	breakatwhitespace=true,
	tabsize=3,
	numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
	numbersep=10pt,                   % how far the line-numbers are from the code
	numberstyle=\tiny\color{customgray}, % the style that is used for the line-numbers
}

\author{
	Mohammad Hossein Shafizadegan\\
	99104781
}
\title{
	Assignment 1 \\
	Computational Intelligence  \\
	Dr. S. Hajipour
}

\pagestyle{fancy}
\rhead{CI}
\lhead{Assignment 1}

\newcommand{\pict}[2]{\begin{center}
		\includegraphics[width=#1\linewidth]{Fig/#2.png}
\end{center}}
\newcommand{\mat}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\deter}[1]{\begin{vmatrix} #1 \end{vmatrix}}

\definecolor{customgreen}{rgb}{0,0.6,0}
\definecolor{customgray}{rgb}{0.5,0.5,0.5}
\definecolor{custommauve}{rgb}{0.6,0,0.8}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
	language=MATLAB,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	frame=single,	                   % adds a frame around the code
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	rulecolor=\color{black},
	breakatwhitespace=true,
	tabsize=3,
	numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
	numbersep=10pt,                   % how far the line-numbers are from the code
	numberstyle=\tiny\color{customgray}, % the style that is used for the line-numbers
}

\begin{document}
	\begin{figure}
		\includegraphics[width=0.25\textwidth]{Fig/Sharif.png}
		\centering
	\end{figure}
	\maketitle
	\tableofcontents
	\newpage
		%-----------------------------------------------------------------------------------------------------------------	
	\section{Question 1}
	\subsection*{a}
	We can define each pixel of the input image as an input for our neural network. Therefor, the total number of input neurons for our network equals \textbf{100}.
	\subsection*{b}
	The number of valid classes that can be assigned as output is \textbf{10} (as for each number from 0 to 9).\\
	If we code the outputs using one-hot encoding, we will need \textbf{10 neurons} for output (for each output class only one neuron has output value of 1).\\
	It is also possible to use simple binary coding for output classes using 4 bits. In this case we only need \textbf{4 output neurons}.
	\subsection*{c}
	The number and size of hidden layers depend on the complexity and non-linearity of the problem. We note that if the number of samples and test data are relatively small its not recommended to have several hidden layers since the convergence won't be achieved. The more hidden layer we have, the more complexity we deal with. Also note that here the inputs can be sparse as some pixels of hand written numbers in the corners are the same for all inputs. The concept of dimensionality reduction can be seen here. 
	
	\subsection*{d}
	In this case, each output of our neural network is value of a particular pixel. The range of this input value is quite vast from 0 to 255. Normalizing input values can make the optimization process faster and more efficient, by preventing the gradients from becoming too large or too small, and by making the error surface smoother and more symmetric.\\\\
	There are several ways to normalize the input. One simple way is called the min-max normalization which transforms the input range from 0 to 255 into 0 to 1. This process can be done by dividing the input by its maximum value. \\
	The other popular method of normalizing is z-score which transfer in a way that the average is equal to 0 and the standard deviation will be 1. This can be achieved by subtracting each data from the average and dividing by the standard deviation.
	\begin{align*}
		\text{min-max normalization : } \frac{x}{\max\{x\}} \qquad , \qquad \text{z-score : } \frac{x - \mu}{\sigma}
	\end{align*}
	
		%-----------------------------------------------------------------------------------------------------------------	
	\section{Question 2}
	\subsection*{a}
	We can use each bit of the input sequence as an input for our neural network hence, we will need \textbf{5 input neurons}. We want to see at the end whether the hamming distance is greater than 2 or not so only \textbf{one output neuron} is enough which produce a binary value indicating the result.
	
	\subsection*{b}
	A linearly separable function is one that can be computed by a single TLU by dividing the input space into two regions with a hyperplane.\\\\
	Although the hamming code can be calculated using XOR which is not linearly separable, in this problem the second code is fixed so there is no need for XOR. In the following section we will show why this problem is linearly separable by providing a TLU as a proof.
	
	\subsection*{c}
	We assume there is a TLU with wieghts of $w_i$ and threshold of $\theta$ that can solve this problem.
	\begin{flalign*}
		&\text{ if } x = 10101 \quad \Rightarrow \quad \sum_{i=1}^{5}w_ix_i = w_1+w_3+w_5 \ge \theta \qquad , \qquad \text{ if } x = 10100 \quad \Rightarrow \quad \sum_{i=1}^{5}w_ix_i = w_1+w_3 \ge \theta&&\\
		&\text{ if } x = 10001 \quad \Rightarrow \quad \sum_{i=1}^{5}w_ix_i = w_1+w_5 \ge \theta \qquad , \qquad \text{ if } x = 00101 \quad \Rightarrow \quad \sum_{i=1}^{5}w_ix_i = w_3+w_5 \ge \theta&&\\
		&\text{ if $x$ has only one "1" } \quad \Rightarrow \quad \sum_{i=1}^{5}w_ix_i = w_i \ge \theta \qquad , \qquad \text{ if } x = 11111 \quad \Rightarrow \quad \sum_{i=1}^{5}w_ix_i \ge \theta&&\\
		&\text{ if } x = 01010 \quad \Rightarrow \quad \sum_{i=1}^{5}w_ix_i = w_2+w_4 < \theta&&\\
		&\text{ assume that } \theta = 2 \qquad \text{based on the above inequalities we set the following values for } w_i \, :&&\\
		&w_1 = w_3 = w_5 = 1 \qquad , \qquad w_2 = w_4 = -1&&
	\end{flalign*}
	we have checked all possible inputs for this network and it can be seen that this TLU works properly.
	\pict{0.25}{F22}
	\subsection*{d}
	The error will occur when the hamming distance of the sample is greater than or equal to 2 but our network's output is 1.
	The error for the train data can be defined as difference of the hamming distance in comparison to 2. We want to keep this distance bellow 2.
		%-----------------------------------------------------------------------------------------------------------------	
	\section{Question 3}
	We first create the karnaugh map for this problem and simplify it as follows : 
	\pict{0.6}{F1}
	\begin{flalign*}
		&y = \overline{x_1} \, x_4 + x_2x_4 + x_1x_3\overline{x_4}&&
	\end{flalign*} 
	
	\subsection*{a}
	\begin{flalign*}
		&\text{AND : } w_i = \begin{cases}
			2 \qquad & \text{ if } l_i = x_i \\
			-2 & \text{ if } l_i = \overline{x_i}
		\end{cases} \qquad , \qquad \theta = n - 1 + \frac{1}{2} \sum_{i=1}^{n} w_i \qquad , \qquad \text{ OR : } w_i = 2 \quad , \quad \theta = 1&&
	\end{flalign*}
	\pict{0.4}{F23}
	
	\subsection*{b}
	We set the thresholds to 0 by adding another weigh for the neurons with value of $-\theta$ and fixed corresponding input of 1.
	\pict{0.4}{F24}
	
	\subsection*{c}
	\begin{center}
		\begin{tabular}{c|c|c|c||c|c|c||c}
			\textbf{$x_1$} & \textbf{$x_2$} & \textbf{$x_3$} & \textbf{$x_4$} & \textbf{$o_1$} & \textbf{$x_2$} & \textbf{$o_3$} & \textbf{$y$} \\
			\hline
			0 & 0.5 & 0 & 0.5 & 0 & 0 & 0 & 0 
		\end{tabular}
	\end{center}
	Based on the above table and following figure, there previous network will have error for this specific input.
	\pict{0.2}{F25}
		%-----------------------------------------------------------------------------------------------------------------	
	\section{Question 4}
	\subsection*{a}
	\begin{flalign*}
		&y = \begin{cases}
			1 \qquad & 2o_1 + 2o_2 \ge 3 \\
			0 & O.W.
		\end{cases} \qquad , \qquad o_1 \,\, , \,\, o_2 2\in \{0, 1\} \quad \Rightarrow \quad \text{ if } y=1 \Rightarrow o_1 = o_2 = 1&&\\
		&\Rightarrow \begin{cases}
			o_1 = 1 \Rightarrow 2x_2 - 2x_1 \ge -1 \\
			o_2 = 1 \Rightarrow 2x_1 - 2x_2 \ge -1
		\end{cases} \quad \Rightarrow \quad \text{ if } x \in (2x_2 - 2x_1 \ge -1) \cap (2x_1 - 2x_2 \ge -1) \Rightarrow y = 1&&
	\end{flalign*}
	\pict{0.3}{F2}
	
	\subsection*{b}
	based on the values of weights and thresholds we first create the truth table as follows :
	\begin{center}
		\begin{tabular}{c|c||c|c||c}
			\textbf{$x_1$} & \textbf{$x_2$} & \textbf{$o_1$} & \textbf{$o_2$} & \textbf{$y$} \\
			\hline
			0 & 0 & 1 & 1 & 1 \\
			\hline
			0 & 1 & 1 & 0 & 0 \\
			\hline
			1 & 1 & 1 & 1 & 1 \\
			\hline
			1 & 0 & 0 & 1 & 0 \\
		\end{tabular}
	\end{center}
	\begin{flalign*}
		&\text{Using SOP format} \Rightarrow y=\overline{x_1} \, \overline{x_2} + x_1x_2 &&
	\end{flalign*}
	It can be seen that this network implements \textbf{XNOR} logical function
	
	\subsection*{c}
	\begin{flalign*}
		&z = \begin{cases}
			1 \qquad & y - x_2 \ge 0 \\
			0 & O.W.
		\end{cases} \quad \Rightarrow \quad x_2 \le y&&\\
		&\text{ if } y = 0 \Rightarrow x_2 \le 0 \qquad , \qquad x \in (2x_2 - 2x_1 < -1) \cup (2x_1 - 2x_2 < -1)&&
	\end{flalign*}
	\pict{0.3}{F3}
	\begin{flalign*}
		&\text{ if } y = 1 \Rightarrow x_2 \le 1 \qquad , \qquad x \in (2x_2 - 2x_1 \ge -1) \cap (2x_1 - 2x_2 \ge -1)&&
	\end{flalign*}
	\pict{0.3}{F4}
	The final answer is aggregation of the above two answers. 
	\pict{0.3}{F5}
	
		%-----------------------------------------------------------------------------------------------------------------	
	\section{Question 5}
	\subsection*{a}
	\pict{0.5}{F21}
	Regarding the above figure, we need 12 TLUs to create each of these lines. Then we need 4 more neurons to perform the AND process to generate each squares. Finally one more TLU is needed to aggregate these 4 sections. The final result for the number of required TLUs is \textbf{17}
	\begin{flalign*}
		&\text{TLUs : } u_1 : -x_1 - x_2 \ge -1 \qquad , \qquad u_2 : x_1 + x_2 \ge -1 \qquad , \qquad u_3 : -x_1 - x_2 \ge 3&&\\
		&u_4 : x_1 + x_2 \ge -5 \qquad , \qquad u_5 : x_1 + x_2 \ge 3 \qquad , \qquad u_6 : -x_1 - x_2 \ge -5&&\\
		&u_7 : x_1 - x_2 \ge -5 \qquad , \qquad u_8 : -x_1 + x_2 \ge 3 \qquad , \qquad u_9 : x_1 - x_2 \ge -1&&\\
		&u_{10} : -x_1 + x_2 \ge -1 \qquad , \qquad u_{11} : x_1 - x_2 \ge 3 \qquad , \qquad u_{12} : -x_1 + x_2 \ge -5&&\\
		&u_{13} \, , \, u_{14} \, , \, u_{15} \, , \, u_{16} \, : \text{ AND TLUs } \Rightarrow w_1 = \dots = w_4 = 2 \quad , \quad \theta = 7&&\\
		&u_{17} \, : \, \text{ OR TLU } : \Rightarrow w_1 = \dots = w_4 = 2 \quad , \quad \theta = 1&&\\\\
		&\text{Ancestor}(u_{13}) = \{u_{11} \, , \, u_{12} \, , \, u_{1} \, , \, u_{2}\} \qquad , \qquad \text{Ancestor}(u_{14}) = \{u_{9} \, , \, u_{10} \, , \, u_{5} \, , \, u_{6}\}&&\\
		&\text{Ancestor}(u_{15}) = \{u_{1} \, , \, u_{2} \, , \, u_{8} \, , \, u_{7}\} \qquad , \qquad \text{Ancestor}(u_{16}) = \{u_{9} \, , \, u_{10} \, , \, u_{3} \, , \, u_{4}\}&&\\
		&\text{Ancestor}(u_{17}) = \{u_{13} \, , \, u_{14} \, , \, u_{15} \, , \, u_{16}\}&&
	\end{flalign*}. 

	\subsection*{b}
	Here we need to rotate and move some of the lines in the previous section by changing the weights and thresholds of the TLUs. Also 4 input TLUs will be removed by setting their weights to 0. The final layer for OR operation is not required anymore.
	
	\subsection*{c}
	In the general case, we need some TLUs to create the lines used for classification in one layer. Then in the following layer we will need some other TLUs to perform the AND operation and generate each closed section. Since the general case can consist of several closed sections (like in section a) we will need another layer to aggregate these section. Therefor, in the general case will require 3 layers. 
	
		%-----------------------------------------------------------------------------------------------------------------	
	\section{Question 6}
	The following neural network consists of only one TLU therefor, it can separate the input space via a straight line into two sections.
	\pict{0.4}{F6}
	The neural network C, first creates two lines. Each one choose a different section of the space. Then each neuron of the hidden layer  defines each of these separate sections and the output layer aggregate them so that the result can be seen as follows
	\pict{0.4}{F7}
	In the neural network D, the first neuron in the first layer creates a vertical line since it only receives $x_1$. The second neuron of the first layer creates a horizontal line since it only receives $x_2$ as input. The output layer choose the common space as it can be seen in the result
	\pict{0.4}{F8}
	In the neural network E, each input neuron creates a straight line since they receive both inputs. The output neuron then finally choose the common chosen section of all these three lines.
	\pict{0.4}{F9}
	In the neural network F, the first neuron of the input layer defines a vertical line as the last neuron of this layer creates a horizontal line since they receive only one input. The middle neuron of this layer can create the diagonal line. The output neuron defines the final section regarding these lines.
	\pict{0.4}{F10}
	There will be only one image that we haven't assign the corresponding neural network. if we set the wight properly for the neural network B, in a way that both input neurons receive only $x_2$, the following image can be accomplished.
	\pict{0.4}{F11}
	
		%-----------------------------------------------------------------------------------------------------------------	
	\section{Question 7}
	\subsection*{a}
	\begin{flalign*}
		&a \,\,  : \,\, x_1+ x_2 = 13 \qquad , \qquad b \,\, : \,\, x_2-x_1 = 13 \qquad , \qquad c \,\, : \,\, x_2 = -7&&\\
		& \Rightarrow \begin{cases}
			\text{blue} = 1 \qquad &  (x_1+x_2 \ge -13)\cap(x_2-x_1 \ge -13)\cap(x_2\ge -7) \\
			\text{red} = 0 & \text{otherwise}
		\end{cases}&&
	\end{flalign*}
	\pict{0.4}{F12}
	We can design the neural network using 3 input neurons and 1 output neuron as follows:
	\pict{0.3}{F19}
	
	\subsection*{b}
	Only one generalized neuron will be enough.
	\begin{flalign*}
		&f_{net} (x_1, x_2) = x_1^2 + y_1^2 \qquad , \qquad f_{act}(net) = \begin{cases}
			1 \qquad & \text{ if } net < 49\\
			0 & \text{otherwise}
		\end{cases} \qquad , \qquad f_{out}(act) = act&&
	\end{flalign*}

	\subsection*{c}
	Using a single generalized neuron, we can create a connected geometric region in the feature space via appropriate equations and formulas for network function, activation and output function . If the TLU network creates this connected, cohesive space, it can be modeled using a single generalized neuron but in the cases that the TLU network consists of several cohesive regions (like question 5), several generalized neurons will be needed for each that regions.
	
	\subsection*{d}
	The pre process includes considering the square of the inputs instead if themselves. 
	\begin{flalign*}
		&\text{define } \quad u = x_1^2 \qquad , \qquad v = x_2^2 \quad \Rightarrow \quad y = \begin{cases}
			1 \qquad & \text{ if } -u-v \ge -49\\
			0 & \text{otherwise} 
		\end{cases}&&
	\end{flalign*}
	In this way, only one single TLU will be enough.
	\pict{0.2}{F20}
	
		%-----------------------------------------------------------------------------------------------------------------	
	\section{Question 8}
	\begin{flalign*}
		&\text{neuron 1} : \qquad z_1 = \exp\left(-\frac{(\max\{| x - 3 |,| x - 5 |,| x + 2 |\})^2}{4}\right) \qquad \Rightarrow&&\\
		&f_{net} = \max\{| x - 3 |,| x - 5 |,| x + 2 |\} \qquad , \qquad f_{act}(net) = \exp\left(-\frac{net^2}{4}\right) \qquad , \qquad f_{out} (act) = act&&\\
		&\text{neuron 2} : \qquad z_2 = \begin{cases}
			0 \qquad & \sqrt{(x_1+3)^2 + (x_3 - 4)^2} > 4\\
			1 & \text{otherwise}
		\end{cases} \qquad \Rightarrow&&\\
		&f_{net} = \sqrt{(x_1+3)^2 + (x_3 - 4)^2} \qquad , \qquad f_{act} = \begin{cases}
			0 \qquad & \text{ if } net \ge 4 \\
			1 & \text{otherwise}
		\end{cases} \qquad , \qquad f_{out} (act) = act&&\\
		&\text{neuron 3} : \qquad y =  6z_1+2z_2+4&&\\
		&f_{net} = 6z_1 + 2z_2 + 4 \qquad , \qquad f_{act} = net \qquad , \qquad f_{out} (act)= act&&
	\end{flalign*}

		%-----------------------------------------------------------------------------------------------------------------	
	\section{Question 9}
	Since the $f_{net}$ can only be a weighted sum function, we have to someway convert $x_1^{x_2}$ into linear combination format. Thus we utilize the logarithmic function. In order to implement the $\max$ function, we have used the following formula :
	\begin{align*}
		\max\{a , b\} = \frac{a + b}{2} + \frac{|a-b|}{2}
	\end{align*}
	We use 6 general neurons as follows:
	\begin{flalign*}
		&\text{neuron 1 : } \text{input : } x_2 \quad , \quad f_{net} = x_2 \quad , \quad f_{act}(net) = \log\left(net\right) \quad , \quad f_{out}(act) = act&&\\
		&\text{neuron 2 : } \text{input : } x_1 \quad , \quad f_{net} = x_1 \quad , \quad f_{act}(net) = \log\left(\log (net)\right) \quad , \quad f_{out}(act) = act&&\\
		&\text{neuron 3 : } \text{input : } out_1 \, , \, out_2 \quad , \quad f_{net} = in_1 + in_2 \quad , \quad f_{act}(net) = \exp(\exp(net)) \quad , \quad f_{out}(act) = act&&\\
		&\text{neuron 4 : } \text{input : } x_3 \, , \, out_3 \quad , \quad f_{net} = in_1 + in_2 \quad , \quad f_{act}(net) = net \quad , \quad f_{out}(act) = act&&\\
		&\text{neuron 5 : } \text{input : } x_3 \, , \, out_3 \quad , \quad f_{net} = in_1 - in_2 \quad , \quad f_{act}(net) = |net| \quad , \quad f_{out}(act) = act&&\\
		&\text{neuron 6 : } \text{input : } out_4 \, , \, out_5 \quad , \quad f_{net} = \frac{in_1}{2} + \frac{in_2}{2} \quad , \quad f_{act}(net) = net \quad , \quad f_{out}(act) = act&&
	\end{flalign*}
	
	%-----------------------------------------------------------------------------------------------------------------	
	\section{Question 10}
	\subsection*{a}
	The wight matrix will be as follows
	\begin{align*}
		\mat{ 0 & 0 & 2 & -2 \\ 2 & 0 & 0 & -3 \\ 1 & -1 & 0 & 4 \\ 0 & 2 & 1 & 0}
	\end{align*}

	\subsection*{b}
	In the input phase the output of the input neurons will be the value of external inputs and the output of the output neurons will be assigned arbitrarily as 0.
	\begin{align*}
		u_1 = 1 \qquad , \qquad u_2 = 1 \qquad , \qquad u_3 = 0 \qquad , \qquad u_4 = 0
	\end{align*}
	Now we start updating the values of the neurons based on the given order
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\textbf{$net_u$} & \textbf{$u_1$} & \textbf{$u_2$} & \textbf{$u_3$} & \textbf{$u_4$} \\
			\hline
			$net_{u_3} = 2u_1 + u_4 = 2$ & $1$ & $1$ & $1$ & $0$ \\
			\hline
			$net_{u_4} = -2u_1 -3u_2 + 4u_3 = -1$ & $1$ & $1$ & 1 & 0 \\
			\hline
			$net_{u_1} = 2u_2 + u_3 = 2$ & $1$ & $1$ & 1 & 0 \\
			\hline
			$net_{u_2} = -u_3 + 2u_4 = -1$ & $1$ & 0 & 1 & 0 \\
			\hline
			$net_{u_3} = 2u_1 + u_4 = 2$ & $1$ & 0 & 1 & 0 \\
			\hline
			$net_{u_4} = -2u_1 -3u_2 + 4u_3 = 2$ & $1$ & 0 & 1 & 1 \\
			\hline
			$net_{u_1} = 2u_2 + u_3 = 1$ & 1 & 0 & 1 & 1 \\
			\hline
			$net_{u_2} = -u_3 + 2u_4 = 1$ & $1$ & 1 & 1 & 0 \\
			\hline
		\end{tabular}
	\end{center}
	
		%-----------------------------------------------------------------------------------------------------------------	
	\section{Question 11}
	\subsection*{a}
	\begin{flalign*}
		w_1x_1 + w_2x_2 \ge \theta \Rightarrow x_1 - x_2 \ge -0.2
	\end{flalign*}
	\pict{0.4}{F13}
	It can be seen that 4 data will be assigned with wrong label
	\subsection*{b}
	\begin{flalign*}
		&x = (0.7 \, , \, 0.65) \qquad , \qquad w = \mat{1 \\ -1} \quad \Rightarrow \quad w^{(new)} = w + \eta \, (o - y) \, x = \mat{1 \\ -1} - \frac{1}{2} \times 2\mat{0.7 \\ 0.65} = \mat{0.3 \\ -1.65}&&\\
		&\theta^{(new)} = \theta - \eta(o-y) = -0.2 + \frac{1}{2} \times 2 = 0.8 \qquad \Rightarrow \qquad 0.3x_1 - 1.65x_2 \ge 0.8&&
	\end{flalign*}
	\pict{0.4}{F14}
	Four data have been labeled incorrectly.
	
	\subsection*{c \& d}
	For this section we have developed a MATLAB code to train this simple neural network. All necessary explanations are provided via comment in the code itself. A section of the code can be seen below. 
	\begin{lstlisting}
		for i=1:n_itter
			y = -1*ones(1,length(x));
			y(w' * x' >= theta) = 1;
			error(:,i) = out - y;
			error_labeled = nonzeros(error(:,i)); % get the nonzero elements
			% stop the process if there is no error :)
			if isempty(error_labeled)
				break;
			else
				% randomly choose one of the data labeled wrongly
				idx = randi(length(error_labeled)); % get a random index
				idx = find(error(:,i) == error_labeled(idx), 1);
				% save the chosen data
				selected_data(i, :) = x(idx, :);
				% update weights and threshold
				w = w + eta * (out(idx) - y(idx)) * x(idx, :)';
				theta = theta - eta * (out(idx) - y(idx));
				fprintf('number of iterations: %d \n', i)
				fprintf('the resulting weigths: w1=%d , w2=%d \n', w(1), w(2));
				fprintf('threshold = %d \n', theta);
				disp('-------------------------------')
			end
		end
	\end{lstlisting}
	Now we set the number of iterations to 4 and run the code. Here we can see the value of weights and threshold in each iteration:
	\pict{0.6}{F15}
	We have created a matrix in our code in order to save the values of errors in each iteration for outputs of all data
	\pict{0.6}{F16}
	We can see that we still facing 4 errors. So now we increase the number of iterations to 100 but we note that in the code, the process will stop whenever the error equals to zero. Here is the final result:
	\pict{0.6}{F17}
	It can be seen that after 15 iterations we won't have any errors. The final value for the weights and the threshold can be also seen above.
	\pict{0.4}{F18}
\end{document}